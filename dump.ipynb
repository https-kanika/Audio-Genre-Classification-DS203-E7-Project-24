{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TO LOAD MIN-MAX POOLED MFCC FILES that are stored in a particular folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mfcc_files(directory):\n",
    "    mfcc_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('-MFCC.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            data = pd.read_csv(file_path, header=None).values\n",
    "            mfcc_data.append(data)\n",
    "    return mfcc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for scalling and building the cdbn model(subject to hyper-parameter tunin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asha Bhosle: 35 files\n",
      "Bhavgeet: 29 files\n",
      "Kishor Kumar: 44 files\n",
      "Lavni: 25 files\n",
      "Michael Jackson: 35 files\n",
      "National Anthem: 29 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_mfcc_files(directory):\n",
    "    mfcc_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        \n",
    "            file_path = os.path.join(directory, file)\n",
    "            data = pd.read_csv(file_path, header=None).values\n",
    "            mfcc_data.append(data)\n",
    "    return mfcc_data\n",
    "\n",
    "# Load MFCC data\n",
    "asha = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Asha Bhosle\")\n",
    "bhavgeet = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Bhavgeet\")\n",
    "kishor = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Kishor Kumar\")\n",
    "lavni = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Lavni\")\n",
    "michael = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Michael Jackson\")\n",
    "national = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\National Anthem\")\n",
    "\n",
    "# Debug: Print the number of files loaded from each directory\n",
    "print(f\"Asha Bhosle: {len(asha)} files\")\n",
    "print(f\"Bhavgeet: {len(bhavgeet)} files\")\n",
    "print(f\"Kishor Kumar: {len(kishor)} files\")\n",
    "print(f\"Lavni: {len(lavni)} files\")\n",
    "print(f\"Michael Jackson: {len(michael)} files\")\n",
    "print(f\"National Anthem: {len(national)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asha Bhosle labeled: 35\n",
      "Bhavgeet labeled: 29\n",
      "Kishor Kumar labeled: 44\n",
      "Lavni labeled: 25\n",
      "Michael Jackson labeled: 35\n",
      "National Anthem labeled: 29\n",
      "(197, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_labeled_data(mfcc_data, label):\n",
    "    labeled_data = []\n",
    "    for data in mfcc_data:\n",
    "        labeled_data.append([data, label])\n",
    "    return labeled_data\n",
    "\n",
    "# Create labeled data\n",
    "asha_labeled = create_labeled_data(asha, 'Asha Bhosle')\n",
    "bhavgeet_labeled = create_labeled_data(bhavgeet, 'Bhavgeet')\n",
    "kishor_labeled = create_labeled_data(kishor, 'Kishor Kumar')\n",
    "lavni_labeled = create_labeled_data(lavni, 'Lavni')\n",
    "michael_labeled = create_labeled_data(michael, 'Michael Jackson')\n",
    "national_labeled = create_labeled_data(national, 'National Anthem')\n",
    "\n",
    "# Debug: Print the length of labeled data for each category\n",
    "print(f\"Asha Bhosle labeled: {len(asha_labeled)}\")\n",
    "print(f\"Bhavgeet labeled: {len(bhavgeet_labeled)}\")\n",
    "print(f\"Kishor Kumar labeled: {len(kishor_labeled)}\")\n",
    "print(f\"Lavni labeled: {len(lavni_labeled)}\")\n",
    "print(f\"Michael Jackson labeled: {len(michael_labeled)}\")\n",
    "print(f\"National Anthem labeled: {len(national_labeled)}\")\n",
    "\n",
    "# Combine all labeled data\n",
    "all_labeled_data = asha_labeled + bhavgeet_labeled + kishor_labeled + lavni_labeled + michael_labeled + national_labeled\n",
    "\n",
    "# Convert to numpy array\n",
    "all_labeled_data_array = np.array(all_labeled_data, dtype=object)\n",
    "\n",
    "# Print the shape of the combined numpy array\n",
    "print(all_labeled_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(all_labeled_data_array[:,1])\n",
    "\n",
    "X=all_labeled_data_array[:,0]\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(X, y_encoded)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kani1\\OneDrive\\Documents\\DS203 Programing for Data Science\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kani1\\OneDrive\\Documents\\DS203 Programing for Data Science\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kani1\\OneDrive\\Documents\\DS203 Programing for Data Science\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "class RBMLayer(layers.Layer):\n",
    "    def __init__(self, n_components, learning_rate=0.01, batch_size=10, n_iter=10, **kwargs):\n",
    "        super(RBMLayer, self).__init__(**kwargs)\n",
    "        self.rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, batch_size=batch_size, n_iter=n_iter)\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs_flat = tf.reshape(inputs, (tf.shape(inputs)[0], -1))\n",
    "        transformed = self.rbm.transform(inputs_flat)\n",
    "        return tf.reshape(transformed, (tf.shape(inputs)[0], self.n_components))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        X_flat = X.reshape((X.shape[0], -1))\n",
    "        self.rbm.fit(X_flat)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_components)\n",
    "    \n",
    "from tensorflow.keras import models\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build the specified model architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data.\n",
    "    \n",
    "    Returns:\n",
    "    tensorflow.keras.Model: The model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Conv1D Layer: 32 filters, filter size of 5, stride of 1\n",
    "    model.add(layers.Conv1D(32, 5, activation='relu', strides=1))\n",
    "    # Pooling Layer: Pool size of 2\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Conv1D Layer: 64 filters, filter size of 5, stride of 1\n",
    "    model.add(layers.Conv1D(64, 5, activation='relu', strides=1))\n",
    "    # Pooling Layer: Pool size of 2\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # RBM Layer 1: 100 hidden units\n",
    "    rbm_layer1 = RBMLayer(n_components=100)\n",
    "    model.add(rbm_layer1)\n",
    "    \n",
    "    # RBM Layer 2: 50 hidden units\n",
    "    rbm_layer2 = RBMLayer(n_components=50)\n",
    "    model.add(rbm_layer2)\n",
    "    \n",
    "    # Fully Connected Layer: 128 units\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "    # Output Layer: Softmax with 6 units for each class\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (147, 1)  # Adjusted to match the input data shape\n",
    "model = build_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to visualize what cdbn is doing (subject to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, layer_name, input_data):\n",
    "    \"\"\"\n",
    "    Visualize the feature maps of a convolutional layer.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The trained model.\n",
    "    layer_name (str): The name of the convolutional layer.\n",
    "    input_data (numpy.ndarray): The input data.\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(name=layer_name)\n",
    "    feature_map_model = models.Model(inputs=model.input, outputs=layer.output)\n",
    "    feature_maps = feature_map_model.predict(input_data)\n",
    "    \n",
    "    n_features = feature_maps.shape[-1]\n",
    "    fig, axes = plt.subplots(1, n_features, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        axes[i].imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_filters(model, layer_name):\n",
    "    \"\"\"\n",
    "    Visualize the filters of a convolutional layer.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The trained model.\n",
    "    layer_name (str): The name of the convolutional layer.\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(name=layer_name)\n",
    "    filters, biases = layer.get_weights()\n",
    "    \n",
    "    # Normalize filter values to 0-1 so we can visualize them\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    n_filters = filters.shape[-1]\n",
    "    fig, axes = plt.subplots(1, n_filters, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        f = filters[:, :, :, i]\n",
    "        axes[i].imshow(f[:, :, 0], cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 20, 25000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = \"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_processed\" #path of the folder which contains the min-max pooled mfcc files\n",
    "mfcc_predict_data = load_mfcc_files(directory)\n",
    "\n",
    "mfcc_predict_data_combined = np.array(mfcc_predict_data)\n",
    "\n",
    "# Print the shape of the combined numpy array\n",
    "print(mfcc_predict_data_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_preprocessed = preprocess_mfcc(mfccs_data_combined)\n",
    "input_shape = mfccs_preprocessed.shape[1:]\n",
    "cdbn_model = build_cdbn(input_shape)\n",
    "cdbn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train = np.array([0])  # Replace with actual data\n",
    "y_train = np.array([0])  # Replace with actual labels\n",
    "\n",
    "# Train the model\n",
    "cdbn_model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "visualize_filters(cdbn_model, 'conv2d')\n",
    "visualize_feature_maps(cdbn_model, 'conv2d', mfccs_preprocessed)\n",
    "\n",
    "# Predict on new data\n",
    "X_test = np.array([0])  # Replace with actual test data\n",
    "predictions = cdbn_model.predict(X_test)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

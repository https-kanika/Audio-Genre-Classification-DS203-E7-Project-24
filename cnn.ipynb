{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mfcc_files(directory):\n",
    "    mfcc_data = []\n",
    "    for file in os.listdir(directory):\n",
    "            print(file)\n",
    "            file_path = os.path.join(directory, file)\n",
    "            data = pd.read_csv(file_path, header=None).values\n",
    "            mfcc_data.append(data)\n",
    "            print(data.shape)\n",
    "    return mfcc_data\n",
    "\n",
    "# Load MFCC data\n",
    "asha = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Asha Bhosle\")\n",
    "bhavgeet = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Bhavgeet\")\n",
    "kishor = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Kishor Kumar\")\n",
    "lavni = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Lavni\")\n",
    "michael = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\Michael Jackson\")\n",
    "national = load_mfcc_files(\"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_train_proccessed\\\\National Anthem\")\n",
    "\n",
    "# Debug: Print the number of files loaded from each directory\n",
    "print(f\"Asha Bhosle: {len(asha)} files\")\n",
    "print(f\"Bhavgeet: {len(bhavgeet)} files\")\n",
    "print(f\"Kishor Kumar: {len(kishor)} files\")\n",
    "print(f\"Lavni: {len(lavni)} files\")\n",
    "print(f\"Michael Jackson: {len(michael)} files\")\n",
    "print(f\"National Anthem: {len(national)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat songs\n",
    "\n",
    "asha = np.array(asha)\n",
    "bhavgeet = np.array(bhavgeet)\n",
    "kishor=np.array(kishor)\n",
    "lavni=np.array(lavni)\n",
    "michael=np.array(michael)\n",
    "national=np.array(national)\n",
    "print(asha.shape)\n",
    "print(bhavgeet.shape)\n",
    "print(kishor.shape)\n",
    "print(lavni.shape)\n",
    "print(michael.shape)\n",
    "print(national.shape)\n",
    "\n",
    "asha=asha[:-1]\n",
    "bhavgeet = bhavgeet[:-1]\n",
    "kishor=kishor[:-1]\n",
    "lavni=lavni[:-3]\n",
    "\n",
    "michael=michael[:-1]\n",
    "print(\"new\")\n",
    "print(asha.shape)\n",
    "print(bhavgeet.shape)\n",
    "print(kishor.shape)\n",
    "print(lavni.shape)\n",
    "print(michael.shape)\n",
    "print(national.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine data\n",
    "data = np.concatenate((asha, bhavgeet, kishor, lavni, michael, national), axis=0)\n",
    "\n",
    "# Create labels\n",
    "labels = ['asha'] * len(asha) + ['bhavgeet'] * len(bhavgeet) + ['kishor'] * len(kishor) + ['lavni'] * len(lavni) + ['michael'] * len(michael) + ['national'] * len(national)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Combine data and labels into a single array\n",
    "combined_data = np.empty((data.shape[0], data.shape[1], data.shape[2] + 1), dtype=object)\n",
    "combined_data[:, :, :-1] = data\n",
    "combined_data[:, 0, -1] = encoded_labels  # Add labels in the last column\n",
    "\n",
    "# Check the shape of the combined data\n",
    "print(f\"Combined data shape: {combined_data.shape}\")\n",
    "\n",
    "#The shape of combined_data will be (294, 20, 25001), where the last element in the innermost dimension contains the encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data without the labels\n",
    "X = combined_data[:, :, :-1]\n",
    "\n",
    "# Extract the labels\n",
    "y = combined_data[:, 0, -1]\n",
    "\n",
    "# Print the shapes of X and y to verify\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print the encoded labels to verify\n",
    "print(f\"Encoded labels: {encoded_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Split the dataset into training and test sets (80% training, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split the training set into training and validation sets (80% training, 20% validation)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "print(x_val[:1])\n",
    "print(x_val[:1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming x_train, x_val, x_test, y_train, y_val, y_test are already defined\n",
    "# and have the shape (176, 20, 25000)\n",
    "\n",
    "# Build the CNN model for feature extraction\n",
    "def build_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(32, 5, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2, padding='same'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    \n",
    "\n",
    "    #model.add(layers.Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.MaxPooling1D(2, padding='same'))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Conv1D(128, 5, activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2, padding='same'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(6, activation='softmax'))  # Assuming 6 classes for classification\n",
    "    return model\n",
    "\n",
    "input_shape = (20, 25000)  # Adjusted to match the input data shape\n",
    "cnn_model = build_cnn_model(input_shape)\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data to match the input shape of the model\n",
    "x_train = x_train.reshape(-1, 20, 25000)\n",
    "x_val = x_val.reshape(-1, 20, 25000)\n",
    "x_test = x_test.reshape(-1, 20, 25000)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "# Train the CNN model with early stopping\n",
    "cnn_model.fit(x_train, y_train, epochs=200, batch_size=16, validation_data=(x_val, y_val), callbacks=[early_stopping,lr_scheduler])\n",
    "\n",
    "\n",
    "# Assuming X_test and y_test are your test data and labels\n",
    "X_test = x_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int32)  # Ensure y_test is integer type for sparse_categorical_crossentropy\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, batch_size=32)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = cnn_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels if necessary\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print detailed classification report\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

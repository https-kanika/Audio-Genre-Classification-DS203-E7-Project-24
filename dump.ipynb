{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TO LOAD MIN-MAX POOLED MFCC FILES that are stored in a particular folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mfcc_files(directory):\n",
    "    mfcc_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('-MFCC.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            data = pd.read_csv(file_path, header=None).values\n",
    "            mfcc_data.append(data)\n",
    "    return mfcc_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for scalling and building the cdbn model(subject to hyper-parameter tunin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mfcc(mfccs):\n",
    "    \"\"\"\n",
    "    Preprocess MFCC features.\n",
    "    \n",
    "    Parameters:\n",
    "    mfccs (numpy.ndarray): The MFCC features.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The preprocessed MFCC features.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    mfccs_scaled = scaler.fit_transform(mfccs)\n",
    "    mfccs_scaled = mfccs_scaled[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions\n",
    "    return mfccs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RBMLayer(layers.Layer):\n",
    "    def __init__(self, n_components, learning_rate=0.01, batch_size=10, n_iter=10, **kwargs):\n",
    "        super(RBMLayer, self).__init__(**kwargs)\n",
    "        self.rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, batch_size=batch_size, n_iter=n_iter)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.rbm.transform(inputs)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.rbm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_rbm_layers(X_train, conv_rbm_layers):\n",
    "    \"\"\"\n",
    "    Pre-train RBM layers on the input data.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): Input data for pre-training.\n",
    "    conv_rbm_layers (list of tuples): List of convolutional RBM layers, each defined by (filters, kernel_size, pool_size, n_components).\n",
    "    \n",
    "    Returns:\n",
    "    list of RBMLayer: List of pre-trained RBM layers.\n",
    "    \"\"\"\n",
    "    rbm_layers = []\n",
    "    for filters, kernel_size, pool_size, n_components in conv_rbm_layers:\n",
    "        # Flatten the input data for RBM training\n",
    "        X_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "        \n",
    "        # Create and fit the RBM layer\n",
    "        rbm_layer = RBMLayer(n_components=n_components)\n",
    "        rbm_layer.fit(X_flat)\n",
    "        \n",
    "        # Transform the input data using the fitted RBM layer\n",
    "        X_train = rbm_layer.call(X_flat).reshape((X_train.shape[0], filters, kernel_size[0], kernel_size[1]))\n",
    "        \n",
    "        rbm_layers.append(rbm_layer)\n",
    "    \n",
    "    return rbm_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cdbn(input_shape, conv_rbm_layers, dense_layers, pre_trained_rbm_layers):\n",
    "    \"\"\"\n",
    "    Build a Convolutional Deep Belief Network (CDBN).\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data.\n",
    "    conv_rbm_layers (list of tuples): List of convolutional RBM layers, each defined by (filters, kernel_size, pool_size, n_components).\n",
    "    dense_layers (list of int): List of dense layer sizes.\n",
    "    pre_trained_rbm_layers (list of RBMLayer): List of pre-trained RBM layers.\n",
    "    \n",
    "    Returns:\n",
    "    tensorflow.keras.Model: The CDBN model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Add convolutional RBM layers\n",
    "    for (filters, kernel_size, pool_size, n_components), rbm_layer in zip(conv_rbm_layers, pre_trained_rbm_layers):\n",
    "        model.add(layers.Conv2D(filters, kernel_size, activation='relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size))\n",
    "        model.add(rbm_layer)\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Add dense layers\n",
    "    for units in dense_layers:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(6, activation='softmax'))  # Adjust the number of classes as needed\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "X_train = np.random.rand(100, 40, 173, 1)  # Replace with actual training data\n",
    "y_train = np.random.randint(0, 6, 100)  # Replace with actual labels\n",
    "\n",
    "# Pre-train RBM layers\n",
    "conv_rbm_layers = [\n",
    "    (32, (3, 3), (2, 2), 64),\n",
    "    (64, (3, 3), (2, 2), 128),\n",
    "    (128, (3, 3), (2, 2), 256)\n",
    "]\n",
    "pre_trained_rbm_layers = pretrain_rbm_layers(X_train, conv_rbm_layers)\n",
    "\n",
    "# Build and compile the CDBN model\n",
    "input_shape = (40, 173, 1)  # Example shape for MFCC (e.g., 40 MFCCs over 173 frames)\n",
    "dense_layers = [128]\n",
    "cdbn_model = build_cdbn(input_shape, conv_rbm_layers, dense_layers, pre_trained_rbm_layers)\n",
    "cdbn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "cdbn_model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to visualize what cdbn is doing (subject to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, layer_name, input_data):\n",
    "    \"\"\"\n",
    "    Visualize the feature maps of a convolutional layer.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The trained model.\n",
    "    layer_name (str): The name of the convolutional layer.\n",
    "    input_data (numpy.ndarray): The input data.\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(name=layer_name)\n",
    "    feature_map_model = models.Model(inputs=model.input, outputs=layer.output)\n",
    "    feature_maps = feature_map_model.predict(input_data)\n",
    "    \n",
    "    n_features = feature_maps.shape[-1]\n",
    "    fig, axes = plt.subplots(1, n_features, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        axes[i].imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_filters(model, layer_name):\n",
    "    \"\"\"\n",
    "    Visualize the filters of a convolutional layer.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The trained model.\n",
    "    layer_name (str): The name of the convolutional layer.\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(name=layer_name)\n",
    "    filters, biases = layer.get_weights()\n",
    "    \n",
    "    # Normalize filter values to 0-1 so we can visualize them\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    n_filters = filters.shape[-1]\n",
    "    fig, axes = plt.subplots(1, n_filters, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        f = filters[:, :, :, i]\n",
    "        axes[i].imshow(f[:, :, 0], cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 20, 25000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = \"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_processed\" #path of the folder which contains the min-max pooled mfcc files\n",
    "mfcc_predict_data = load_mfcc_files(directory)\n",
    "\n",
    "mfcc_predict_data_combined = np.array(mfcc_predict_data)\n",
    "\n",
    "# Print the shape of the combined numpy array\n",
    "print(mfcc_predict_data_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_preprocessed = preprocess_mfcc(mfccs_data_combined)\n",
    "input_shape = mfccs_preprocessed.shape[1:]\n",
    "cdbn_model = build_cdbn(input_shape)\n",
    "cdbn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train = np.array([0])  # Replace with actual data\n",
    "y_train = np.array([0])  # Replace with actual labels\n",
    "\n",
    "# Train the model\n",
    "cdbn_model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "visualize_filters(cdbn_model, 'conv2d')\n",
    "visualize_feature_maps(cdbn_model, 'conv2d', mfccs_preprocessed)\n",
    "\n",
    "# Predict on new data\n",
    "X_test = np.array([0])  # Replace with actual test data\n",
    "predictions = cdbn_model.predict(X_test)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

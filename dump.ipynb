{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TO LOAD MIN-MAX POOLED MFCC FILES that are stored in a particular folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mfcc_files(directory):\n",
    "    mfcc_data = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('-MFCC.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            data = pd.read_csv(file_path, header=None).values\n",
    "            mfcc_data.append(data)\n",
    "    return mfcc_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for scalling and building the cdbn model(subject to hyper-parameter tunin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mfcc(mfccs):\n",
    "    \"\"\"\n",
    "    Preprocess MFCC features.\n",
    "    \n",
    "    Parameters:\n",
    "    mfccs (numpy.ndarray): The MFCC features.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The preprocessed MFCC features.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    mfccs_scaled = scaler.fit_transform(mfccs)\n",
    "    mfccs_scaled = mfccs_scaled[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions\n",
    "    return mfccs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kani1\\OneDrive\\Documents\\DS203 Programing for Data Science\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BernoulliRBM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 51\u001b[0m\n\u001b[0;32m     44\u001b[0m conv_rbm_layers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     45\u001b[0m     (\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     46\u001b[0m     (\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m128\u001b[39m),\n\u001b[0;32m     47\u001b[0m     (\u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     48\u001b[0m ]\n\u001b[0;32m     49\u001b[0m dense_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m cdbn_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_cdbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_rbm_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m cdbn_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m, in \u001b[0;36mbuild_cdbn\u001b[1;34m(input_shape, conv_rbm_layers, dense_layers)\u001b[0m\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mConv2D(filters, kernel_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mMaxPooling2D(pool_size))\n\u001b[1;32m---> 31\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mRBMLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mFlatten())\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Add dense layers\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mRBMLayer.__init__\u001b[1;34m(self, n_components, learning_rate, batch_size, n_iter, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_components, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(RBMLayer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbm \u001b[38;5;241m=\u001b[39m \u001b[43mBernoulliRBM\u001b[49m(n_components\u001b[38;5;241m=\u001b[39mn_components, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, n_iter\u001b[38;5;241m=\u001b[39mn_iter)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BernoulliRBM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class RBMLayer(layers.Layer):\n",
    "    def __init__(self, n_components, learning_rate=0.01, batch_size=10, n_iter=10, **kwargs):\n",
    "        super(RBMLayer, self).__init__(**kwargs)\n",
    "        self.rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, batch_size=batch_size, n_iter=n_iter)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.rbm.transform(inputs)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.rbm.fit(X)\n",
    "\n",
    "def build_cdbn(input_shape, conv_rbm_layers, dense_layers):\n",
    "    \"\"\"\n",
    "    Build a Convolutional Deep Belief Network (CDBN).\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data.\n",
    "    conv_rbm_layers (list of tuples): List of convolutional RBM layers, each defined by (filters, kernel_size, pool_size, n_components).\n",
    "    dense_layers (list of int): List of dense layer sizes.\n",
    "    \n",
    "    Returns:\n",
    "    tensorflow.keras.Model: The CDBN model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Add convolutional RBM layers\n",
    "    for filters, kernel_size, pool_size, n_components in conv_rbm_layers:\n",
    "        model.add(layers.Conv2D(filters, kernel_size, activation='relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size))\n",
    "        model.add(RBMLayer(n_components=n_components))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Add dense layers\n",
    "    for units in dense_layers:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(6, activation='softmax'))  # Adjust the number of classes as needed\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (40, 173, 1)  # Example shape for MFCC (e.g., 40 MFCCs over 173 frames)\n",
    "conv_rbm_layers = [\n",
    "    (32, (3, 3), (2, 2), 64),\n",
    "    (64, (3, 3), (2, 2), 128),\n",
    "    (128, (3, 3), (2, 2), 256)\n",
    "]\n",
    "dense_layers = [128]\n",
    "\n",
    "cdbn_model = build_cdbn(input_shape, conv_rbm_layers, dense_layers)\n",
    "cdbn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to visualize what cdbn is doing (subject to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, layer_name, input_data):\n",
    "    \"\"\"\n",
    "    Visualize the feature maps of a convolutional layer.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The trained model.\n",
    "    layer_name (str): The name of the convolutional layer.\n",
    "    input_data (numpy.ndarray): The input data.\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(name=layer_name)\n",
    "    feature_map_model = models.Model(inputs=model.input, outputs=layer.output)\n",
    "    feature_maps = feature_map_model.predict(input_data)\n",
    "    \n",
    "    n_features = feature_maps.shape[-1]\n",
    "    fig, axes = plt.subplots(1, n_features, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        axes[i].imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_filters(model, layer_name):\n",
    "    \"\"\"\n",
    "    Visualize the filters of a convolutional layer.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The trained model.\n",
    "    layer_name (str): The name of the convolutional layer.\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(name=layer_name)\n",
    "    filters, biases = layer.get_weights()\n",
    "    \n",
    "    # Normalize filter values to 0-1 so we can visualize them\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    n_filters = filters.shape[-1]\n",
    "    fig, axes = plt.subplots(1, n_filters, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        f = filters[:, :, :, i]\n",
    "        axes[i].imshow(f[:, :, 0], cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 20, 25000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = \"C:\\\\Users\\\\kani1\\\\OneDrive\\\\Documents\\\\DS203 Programing for Data Science\\\\project\\\\mfcc_processed\" #path of the folder which contains the min-max pooled mfcc files\n",
    "mfcc_predict_data = load_mfcc_files(directory)\n",
    "\n",
    "mfcc_predict_data_combined = np.array(mfcc_predict_data)\n",
    "\n",
    "# Print the shape of the combined numpy array\n",
    "print(mfcc_predict_data_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_preprocessed = preprocess_mfcc(mfccs_data_combined)\n",
    "input_shape = mfccs_preprocessed.shape[1:]\n",
    "cdbn_model = build_cdbn(input_shape)\n",
    "cdbn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train = np.array([0])  # Replace with actual data\n",
    "y_train = np.array([0])  # Replace with actual labels\n",
    "\n",
    "# Train the model\n",
    "cdbn_model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "visualize_filters(cdbn_model, 'conv2d')\n",
    "visualize_feature_maps(cdbn_model, 'conv2d', mfccs_preprocessed)\n",
    "\n",
    "# Predict on new data\n",
    "X_test = np.array([0])  # Replace with actual test data\n",
    "predictions = cdbn_model.predict(X_test)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

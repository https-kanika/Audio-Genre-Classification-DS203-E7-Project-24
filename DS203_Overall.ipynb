{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS203- E7 Project\n",
    "\n",
    "This code file contains all the codes which we ran to do specific operations on the data to get to out final results.\n",
    "Use and working of each code is written as we go ahead in the code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import IPython.display as ipd\n",
    "import glob\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MFCC Generation\n",
    "\n",
    "1. **MFCCs (Mel-Frequency Cepstral Coefficients)**:\n",
    "   - MFCCs are essential audio features that summarize the frequency content of sound. They are widely used in audio processing tasks because they capture characteristics relevant to human auditory perception.\n",
    "   - By compressing frequency data into a smaller, informative set of coefficients, MFCCs make it easier to classify and analyze audio, especially for machine learning models in speech and music classification.\n",
    "   \n",
    "2. **Sampling Rate**:\n",
    "   - Sets a 44100 Hz rate for consistent audio quality, essential for uniform feature extraction across files and reliable model learning.\n",
    "\n",
    "3. **Data Preparation**:\n",
    "   - Saves MFCCs as CSVs, creating a structured dataset ideal for machine learning applications in audio analysis, like classification and pattern recognition.\n",
    "\n",
    "4. **Batch Processing**:\n",
    "   - Processes all files in the directory automatically, enabling scalable dataset creation for research and industry.\n",
    "\n",
    "5. **Error Handling**:\n",
    "   - Handles errors per file to allow smooth processing, even with large or mixed-quality audio datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the folder containing audio files and the output directory\n",
    "audio_folder = r'C:\\Users\\Dnyaneshwari\\Desktop\\song'\n",
    "output_directory = r'C:\\Users\\Dnyaneshwari\\Desktop\\01-mfcc'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Set sampling rate and number of MFCCs to extract\n",
    "sr_value = 44100\n",
    "n_mfcc_count = 20\n",
    "\n",
    "# Loop over all files in the audio folder\n",
    "for audio_file in os.listdir(audio_folder):\n",
    "    # Construct the full file path\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    \n",
    "    # Check if the file is an audio file (assuming .mp3 format here)\n",
    "    if audio_path.endswith('.mp3'):\n",
    "        try:\n",
    "            # Load the audio file\n",
    "            y, sr = librosa.load(audio_path, sr=sr_value)\n",
    "            \n",
    "            # Extract MFCCs\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc_count)\n",
    "            \n",
    "            # MFCCs shape is (n_mfcc_count, num_frames)\n",
    "            # # Transpose it to get 20 rows (MFCC features) and as many columns as frames\n",
    "            # mfccs = mfccs.transpose()\n",
    "            \n",
    "            # Check the shape (optional)\n",
    "            print(f\"MFCC shape for {audio_file}: {mfccs.shape}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            coeff_df = pd.DataFrame(mfccs)\n",
    "            \n",
    "            # Generate a CSV filename based on the audio file name\n",
    "            csv_filename = os.path.splitext(os.path.basename(audio_path))[0] + '_mfcc.csv'\n",
    "            csv_file_path = os.path.join(output_directory, csv_filename)\n",
    "            \n",
    "            # Save the DataFrame to a CSV file\n",
    "            coeff_df.to_csv(csv_file_path, index=False, header=False)\n",
    "\n",
    "            \n",
    "            print(f'MFCC coefficients saved to {csv_file_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {audio_file}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Refinement\n",
    "\n",
    "We tried to listen some of the songs to understand how does the MFCC to audio conversion makes changes into the actual songs. The songs werent very clear, so we tries to explore some audio refinement techniques to make the songs more clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Summary\n",
    "\n",
    "1. **invert_MFCC_to_audio**: Converts MFCC coefficients back into audio using `librosa`, applying the Griffin-Lim algorithm for better quality.\n",
    "\n",
    "2. **enhance_audio**: Uses `noisereduce` to reduce noise in the audio signal, improving clarity.\n",
    "\n",
    "3. **save_audio_file**: Saves the enhanced audio as a `.wav` file.\n",
    "\n",
    "4. **convert_mfcc_csv_to_audio**: Reads MFCC data from a CSV, converts it to audio, enhances it, and saves it to a specified directory.\n",
    "\n",
    "### Conceptual Relevance\n",
    "\n",
    "- **MFCC Inversion**: Reconstructs audio from MFCCs, enabling feature-based analysis.\n",
    "- **Noise Reduction**: Improves the quality of the reconstructed audio.\n",
    "- **Automation**: Processes MFCC files in batch and saves the results for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def invert_MFCC_to_audio(mfcc_coefficients, sr=44100, n_iter=32):\n",
    "    \"\"\"\n",
    "    Invert MFCC coefficients back to audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    mfcc_coefficients (numpy.ndarray): The MFCC coefficients.\n",
    "    sr (int): The sampling rate of the audio.\n",
    "    n_iter (int): Number of iterations for the Griffin-Lim algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The reconstructed audio signal.\n",
    "    \"\"\"\n",
    "    mfcc_coefficients = np.nan_to_num(mfcc_coefficients)\n",
    "    mel_spectrogram = librosa.feature.inverse.mfcc_to_mel(mfcc_coefficients)\n",
    "    mel_spectrogram = np.nan_to_num(mel_spectrogram)\n",
    "    audio_signal = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr, n_iter=n_iter)\n",
    "    return np.nan_to_num(audio_signal)\n",
    "\n",
    "def enhance_audio(audio_signal, sr=44100):\n",
    "    \"\"\"\n",
    "    Enhance audio quality by aggressively reducing noise.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_signal (numpy.ndarray): The audio signal.\n",
    "    sr (int): Sampling rate of the audio signal.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The enhanced audio signal.\n",
    "    \"\"\"\n",
    "    return nr.reduce_noise(y=audio_signal, sr=sr, stationary=False, prop_decrease=1.0)\n",
    "\n",
    "def save_audio_file(audio_signal, output_file_path, sr=44100):\n",
    "    \"\"\"\n",
    "    Save audio signal to a .wav file.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_signal (numpy.ndarray): The audio signal to save.\n",
    "    output_file_path (str): The path where the audio file will be saved.\n",
    "    sr (int): Sampling rate of the audio signal.\n",
    "    \"\"\"\n",
    "    if audio_signal is not None and len(audio_signal) > 0:\n",
    "        sf.write(output_file_path, audio_signal, sr)\n",
    "        print(f\"Saved audio to {output_file_path}.\")\n",
    "    else:\n",
    "        print(f\"Warning: No valid audio signal to save for {output_file_path}.\")\n",
    "\n",
    "def convert_mfcc_csv_to_audio(csv_file, output_dir='extracted_audio', sr=44100):\n",
    "    \"\"\"\n",
    "    Convert MFCC features from a CSV file to audio files.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing MFCC features.\n",
    "    output_dir (str): Directory where audio files will be saved.\n",
    "    sr (int): Sampling rate of the audio signal.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    mfcc_data = pd.read_csv(csv_file, header=None)\n",
    "    mfccs = mfcc_data.values.flatten()\n",
    "\n",
    "    # Reshape the MFCC coefficients to a 2D array (assuming 20 MFCCs per frame)\n",
    "    n_mfcc = 20\n",
    "    mfccs = mfccs.reshape((n_mfcc, -1))\n",
    "\n",
    "    # Invert MFCC to audio\n",
    "    audio_signal = invert_MFCC_to_audio(mfccs, sr=sr)\n",
    "\n",
    "    # Enhance the audio signal\n",
    "    enhanced_audio = enhance_audio(audio_signal, sr=sr)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the output filename and save the audio\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(csv_file).replace('.csv', '_audio.wav'))\n",
    "    save_audio_file(enhanced_audio, output_file_path, sr=sr)\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = r\"C:\\Users\\mahim\\Desktop\\DS203 FILES\\MFCCCC\\89-MFCC.csv\"  # Path to your MFCC CSV file\n",
    "convert_mfcc_csv_to_audio(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA : checking for missing values in csv files\n",
    "We didnt find any missing values in the provided dataset of mfcc csv files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the directory containing your MFCC CSV files\n",
    "data_path = 'C:/Users/mahim/Desktop/DS203 FILES/MFCCCC'\n",
    "\n",
    "# Step 3: Load all CSV files into a list of DataFrames\n",
    "mfcc_data = []\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(data_path, file))\n",
    "        mfcc_data.append(df)\n",
    "\n",
    "\n",
    "# Initialize a list to store the count of dropped rows for each file\n",
    "dropped_rows_count = []\n",
    "\n",
    "for i in range(0, 115):\n",
    "    # Find rows with missing values\n",
    "    rows_with_na = mfcc_data[i][mfcc_data[i].isna().any(axis=1)]\n",
    "    \n",
    "    # Count the rows that will be dropped\n",
    "    num_dropped_rows = len(rows_with_na)\n",
    "    dropped_rows_count.append(num_dropped_rows)\n",
    "    \n",
    "    # Print the indices and data of rows that had missing values\n",
    "    print(f\"\\nFile {i + 1} - Rows with missing values (indices):\")\n",
    "    print(rows_with_na.index.tolist())\n",
    "    print(\"\\nData of rows that were dropped:\")\n",
    "    print(rows_with_na)\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    mfcc_data[i].dropna(inplace=True)\n",
    "\n",
    "# Print the total number of rows dropped per file\n",
    "for i, count in enumerate(dropped_rows_count, start=1):\n",
    "    print(f\"File {i} - Rows dropped: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Max pooling\n",
    "\n",
    "Mean and Max Pooling: Both techniques reduce the spatial dimensions of the input data, making the model more computationally efficient. While max pooling emphasizes the most important features by retaining the maximum values, mean pooling helps retain global contextual information by averaging. Together, they provide a balance of focusing on key features while preserving overall structure, making them effective for improving model robustness and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Gather column counts from all files\n",
    "def gather_column_counts(file_paths):\n",
    "    column_counts = []\n",
    "    for file in file_paths:\n",
    "        data = pd.read_csv(file)\n",
    "        column_counts.append(data.shape[1])\n",
    "    return column_counts\n",
    "\n",
    "# Step 2: Perform EDA to decide on target column count\n",
    "def analyze_column_counts(column_counts):\n",
    "    # Display statistical measures\n",
    "    percentiles = np.percentile(column_counts, [25, 50, 75, 90])\n",
    "    iqr = percentiles[2] - percentiles[0]\n",
    "    print(\"Column Count Statistics:\")\n",
    "    print(f\"25th Percentile: {percentiles[0]}\")\n",
    "    print(f\"Median (50th Percentile): {percentiles[1]}\")\n",
    "    print(f\"75th Percentile: {percentiles[2]}\")\n",
    "    print(f\"90th Percentile: {percentiles[3]}\")\n",
    "    print(f\"Interquartile Range (IQR): {iqr}\")\n",
    "    \n",
    "    # Plot histogram and boxplot\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(column_counts, kde=True)\n",
    "    plt.title(\"Distribution of Column Counts\")\n",
    "    plt.xlabel(\"Number of Columns\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=column_counts)\n",
    "    plt.title(\"Boxplot of Column Counts\")\n",
    "    plt.xlabel(\"Number of Columns\")\n",
    "    plt.show()\n",
    "\n",
    "# File paths for MFCC CSV files\n",
    "file_paths = glob.glob('C:/Users/mahim/Desktop/DS203 FILES/MFCCCC/*.csv')\n",
    "\n",
    "# Step 3: Gather and analyze column counts\n",
    "column_counts = gather_column_counts(file_paths)\n",
    "analyze_column_counts(column_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the the statistics and data distribution, We choose the fixed size as 20 rows and 25000 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def mean_max_pooling(data, target_size):\n",
    "    current_size = data.shape[1]\n",
    "    \n",
    "    if current_size > target_size:\n",
    "        # Downsampling by mean-max pooling\n",
    "        factor = current_size / target_size\n",
    "        pooled_data = []\n",
    "        \n",
    "        for i in range(target_size):\n",
    "            start_idx = int(i * factor)\n",
    "            end_idx = int((i + 1) * factor)\n",
    "            segment = data.iloc[:, start_idx:end_idx]\n",
    "            \n",
    "            # Calculate mean and max for the segment\n",
    "            mean_values = segment.mean(axis=1)\n",
    "            max_values = segment.max(axis=1)\n",
    "            # Combine mean and max to form the pooled result\n",
    "            combined_values = (mean_values + max_values) / 2\n",
    "            pooled_data.append(combined_values)\n",
    "        \n",
    "        data_resized = pd.DataFrame(pooled_data).T\n",
    "    \n",
    "    elif current_size < target_size:\n",
    "        # Upsampling by padding\n",
    "        padding = pd.DataFrame(0, index=data.index, columns=range(target_size - current_size))\n",
    "        data_resized = pd.concat([data, padding], axis=1)\n",
    "    else:\n",
    "        # No resizing needed\n",
    "        data_resized = data\n",
    "    \n",
    "    return data_resized\n",
    "\n",
    "def resize_mfcc_files(input_dir, output_dir, target_size):\n",
    "    # Get all CSV files in the input directory\n",
    "    file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "    if not file_paths:\n",
    "        print(\"No CSV files found in the specified directory.\")\n",
    "        return\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, header=None)\n",
    "        \n",
    "        # Apply mean-max pooling to reach the target size\n",
    "        data_resized = mean_max_pooling(data, target_size)\n",
    "        \n",
    "        # Save the resized data to the output directory\n",
    "        file_name = os.path.basename(file_path)\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "        data_resized.to_csv(output_path, index=False, header=False)\n",
    "        print(f\"Resized and saved: {output_path}\")\n",
    "\n",
    "# Parameters\n",
    "input_dir = \"C:/Users/mahim/Downloads/mfcc/mfcc/mj\"   # Directory containing the original MFCC CSV files\n",
    "output_dir = \"C:/Users/mahim/Desktop/DS203 FILES/michaell\" # Directory to save the resized CSV files\n",
    "target_size = 25000  # Set your chosen target column size here\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Resize all files to the target size\n",
    "resize_mfcc_files(input_dir, output_dir, target_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied this to all the provided datasets(test) as well as our created training dataset (the files we downloaded to train the model) so that we have a uniform fixed size for all files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "--To increase our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Directory where your MFCC CSV files are located\n",
    "input_dir = r\"C:\\Users\\Dnyaneshwari\\Desktop\\mahima files\\Kishor Kumar-20241106T100635Z-001\\Kishor Kumar\"   # Replace with the path to your original 50 MFCC files\n",
    "output_dir = r'C:\\Users\\Dnyaneshwari\\Desktop\\mahima files\\kk-aug'   # Replace with the path to save augmented files\n",
    "\n",
    "# Augmentation settings\n",
    "scaling_factor_min = 0.9  # Min scaling factor\n",
    "scaling_factor_max = 1.1  # Max scaling factor\n",
    "time_shift_max = 5        # Max number of frames to shift\n",
    "frame_removal_prob = 0.1  # Probability of removing a frame\n",
    "num_augmented_files = 10  # Number of augmented files per original file\n",
    "fixed_num_rows = 20       # Ensure output has exactly 20 rows\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def scale_mfcc(mfcc_data, scaling_factor_min=0.9, scaling_factor_max=1.1):\n",
    "    \"\"\"Scale MFCC values by a random factor.\"\"\"\n",
    "    scale = random.uniform(scaling_factor_min, scaling_factor_max)\n",
    "    return mfcc_data * scale\n",
    "\n",
    "def time_shift(mfcc_data, shift_max=5):\n",
    "    \"\"\"Randomly shift the MFCC frames.\"\"\"\n",
    "    shift = random.randint(-shift_max, shift_max)\n",
    "    if shift > 0:\n",
    "        return np.pad(mfcc_data, ((shift, 0), (0, 0)), mode='constant')[:-shift, :]\n",
    "    elif shift < 0:\n",
    "        return np.pad(mfcc_data, ((0, -shift), (0, 0)), mode='constant')[:shift, :]\n",
    "    else:\n",
    "        return mfcc_data\n",
    "\n",
    "def remove_random_frames(mfcc_data, frame_removal_prob=0.1):\n",
    "    \"\"\"Randomly remove frames from the MFCC data.\"\"\"\n",
    "    num_frames = mfcc_data.shape[0]\n",
    "    keep_indices = [i for i in range(num_frames)]\n",
    "    \n",
    "    # Randomly remove frames based on the probability\n",
    "    for i in range(num_frames):\n",
    "        if random.random() < frame_removal_prob:\n",
    "            keep_indices.remove(i)\n",
    "\n",
    "    return mfcc_data[keep_indices]\n",
    "\n",
    "def ensure_fixed_num_rows(mfcc_data, fixed_num_rows=20):\n",
    "    \"\"\"Ensure that MFCC data has exactly a fixed number of rows.\"\"\"\n",
    "    current_num_rows = mfcc_data.shape[0]\n",
    "    if current_num_rows > fixed_num_rows:\n",
    "        # Truncate rows if there are more than fixed_num_rows\n",
    "        return mfcc_data[:fixed_num_rows, :]\n",
    "    elif current_num_rows < fixed_num_rows:\n",
    "        # Pad rows with zeros if there are fewer than fixed_num_rows\n",
    "        padding = np.zeros((fixed_num_rows - current_num_rows, mfcc_data.shape[1]))\n",
    "        return np.vstack((mfcc_data, padding))\n",
    "    else:\n",
    "        return mfcc_data\n",
    "\n",
    "def augment_mfcc(mfcc_data):\n",
    "    \"\"\"Apply a series of augmentations to MFCC data.\"\"\"\n",
    "    augmentation_choice = random.choice(['scale', 'shift', 'remove_frames'])\n",
    "    \n",
    "    if augmentation_choice == 'scale':\n",
    "        mfcc_data = scale_mfcc(mfcc_data, scaling_factor_min, scaling_factor_max)\n",
    "    elif augmentation_choice == 'shift':\n",
    "        mfcc_data = time_shift(mfcc_data, time_shift_max)\n",
    "    elif augmentation_choice == 'remove_frames':\n",
    "        mfcc_data = remove_random_frames(mfcc_data, frame_removal_prob)\n",
    "    \n",
    "    # Ensure the data has a fixed number of rows\n",
    "    return ensure_fixed_num_rows(mfcc_data, fixed_num_rows)\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Process each MFCC CSV file and apply augmentations.\"\"\"\n",
    "    try:\n",
    "        # Read the original MFCC CSV file\n",
    "        mfcc_data = pd.read_csv(input_file, header=None).values  # Assuming no header\n",
    "\n",
    "        # Apply augmentation to the MFCC data\n",
    "        augmented_data = augment_mfcc(mfcc_data)\n",
    "\n",
    "        # Save the augmented MFCC data to a new CSV file\n",
    "        np.savetxt(output_file, augmented_data, delimiter=',', fmt='%.6f')\n",
    "        print(f\"Processed and saved: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "def process_all_files(input_dir, output_dir, num_augmented_files=10):\n",
    "    \"\"\"Process all MFCC CSV files in the input directory and generate multiple augmented files.\"\"\"\n",
    "    # Get all CSV files in the input directory\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    # Loop through each file and generate multiple augmented versions\n",
    "    for file in files:\n",
    "        input_file = os.path.join(input_dir, file)\n",
    "        \n",
    "        # Generate 'num_augmented_files' augmented versions of the file\n",
    "        for i in range(num_augmented_files):\n",
    "            output_file = os.path.join(output_dir, f\"aug_{i+1}_{file}\")\n",
    "            process_file(input_file, output_file)\n",
    "\n",
    "# Start processing all files and generating augmented data\n",
    "process_all_files(input_dir, output_dir, num_augmented_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing individual MFCC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming mfcc is your MFCC data with shape (num_samples, num_mfcc, num_frames)\n",
    "# For example, let's create a dummy MFCC array for demonstration\n",
    "\n",
    "mfcc = x_train[:1]\n",
    "\n",
    "# Visualize the MFCCs for a single sample\n",
    "def visualize_mfcc(mfcc, sample_index=0):\n",
    "    \"\"\"\n",
    "    Visualize the MFCCs for a single sample.\n",
    "    \n",
    "    Parameters:\n",
    "    mfcc (numpy.ndarray): The MFCC data with shape (num_samples, num_mfcc, num_frames).\n",
    "    sample_index (int): The index of the sample to visualize.\n",
    "    \"\"\"\n",
    "    mfcc_sample = mfcc[sample_index]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mfcc_sample, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('MFCC')\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the MFCCs for the first sample\n",
    "visualize_mfcc(mfcc, sample_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

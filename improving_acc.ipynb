{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2. Model Architecture\n",
    "'''\n",
    "*Adding More Layers:*\n",
    "python\n",
    "'''\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3. Regularization\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "*L2 Regularization:*\n",
    "python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 4. Optimization\n",
    "\n",
    "*Learning Rate Schedulers:*\n",
    "python\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "model.fit(x_train, y_train, epochs=37, batch_size=40, callbacks=[callback])\n",
    "\n",
    "'''\n",
    "*Advanced Optimizers:*\n",
    "'''\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "model.compile(optimizer=AdamW(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''5. Cross-Validation\n",
    "\n",
    "*K-Fold Cross-Validation:*\n",
    "python'''\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    model.fit(x_train_fold, y_train_fold, validation_data=(x_val_fold, y_val_fold), epochs=37, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 6. Feature Engineering\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "x_train = selector.fit_transform(x_train, y_train)\n",
    "x_test = selector.transform(x_test)\n",
    "\n",
    "'''PCA'''\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''### 7. Ensemble Methods\n",
    "\n",
    "*Ensemble Learning (Voting Classifier):*\n",
    "python'''\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('svc', SVC(probability=True))\n",
    "], voting='soft')\n",
    "\n",
    "ensemble_model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### 8. Early Stopping\n",
    "\n",
    "*Early Stopping:*\n",
    "python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(x_train, y_train, epochs=37, batch_size=40, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "### 9. Data Quality\n",
    "\n",
    "*Clean Data*: Ensure your data is clean and free from errors or inconsistencies. This is more of a manual process depending on your dataset.\n",
    "\n",
    "*Increase Data*: If possible, gather more data to train your model.\n",
    "\n",
    "### 10. Transfer Learning\n",
    "\n",
    "*Using Pre-trained Models:*\n",
    "python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "Feel free to integrate these snippets into your code and adjust them as needed for your specific use case. If you have any questions or need further assistance, let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS203_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
